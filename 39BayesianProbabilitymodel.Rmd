贝叶斯概率模型

概念解释
    P(A|B)=p(B|A)p(A)/p(B)
    从理论上讲，贝叶斯概率模型是一种典型的基于结果来求解原因的模型，其中p(A)为先验概率，表示每种类别分布的概率；p(B|A)为类条件概率（也称似然概率），表示在某种类别的前提下，某事发生的概率；而P(A|B)为后验概率，后验概率越大，说明某事物属于这个类别的可能性越大。

朴素贝叶斯
    在统计学和计算机科学的文献中，提到的通常为贝叶斯概率模型的简化模型朴素贝叶斯，又称简单贝叶斯或独立贝叶斯。本质上，朴素贝叶斯是一种生成式模型，是在贝叶斯算法上做了简化，即假设定给定目标值的条件下，各个特征相互独立，这个简化使得各个特征对分类结果的影响较为平均，一定程度上降低了分类器的分类效果，但是极大的简化了算法的复杂度。朴素贝叶斯自20世纪50年代起被广泛研究，是文本分类的一种热门方法，在实际应用中往往能够取得相当好的结果，被广泛应用于对复杂的现实情况的分析当中。

贝叶斯分类器    
    在多数情况下，人们在提到朴素贝叶斯时，更多是指建立在朴素贝叶斯概率模型上的贝叶斯分类器。在机器学习中，朴素贝叶斯分类器是一系列以假设特征之间强独立下运用贝叶斯定理为基础的简单概率分类器。所有的朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。在实际应用中，朴素贝叶斯模型参数估计使用最大似然估计的方法，也就是说，即便在不使用贝叶斯概率或任何贝叶斯模型的情况下，朴素贝叶斯也能取得很好的效果。
    假设某个体有n项特征，分别为F1、F2、...、Fn。现有m个类别，分别为C1、C2、...、Cm。贝叶斯分类器就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：
    P(C|F1F2...Fn)= P(F1F2...Fn|C)P(C) / P(F1F2...Fn)
又由于 P(F1F2...Fn)对于所有的类别均相同，于是就变为求P(F1F2...Fn|C)P(C)
的最大值。
    而朴素贝叶斯分类器又假设所有特征都彼此独立，因此P(F1F2...Fn|C)P(C)= P(F1|C)P(F2|C) ... P(Fn|C)P(C)
    由于概率模型被分解为类先验概率P(C)与独立概率分布P(Fi|C)，概率模型的可控性得到了极大的提高，所需要估计的参数数量也得到了极大的减少，因此朴素贝叶斯分类器的一个优势在于只需要根据少量的训练数据就可以估计出必要的参数，且由于变量之间独立假设，只需要估计各个变量的方法，而并不需要确定整个协方差矩阵。

参数估计   
    在实际问题当中，我们往往只能获取到有限数目的样本数据，而先验概率和类条件概率(各类的总体分布)都是未知的。根据仅有的样本数据进行分类时，通常需要我们对先验概率和类条件概率进行估计。
    先验概率的估计一般来说较为简单，常用方法是概率的最大似然估计。类的先验概率可以通过假设各类等概率来计算（先验概率 = 1 / (类的数量)），或者通过训练集的各类样本出现的次数来估计（A类先验概率=（A类样本的数量）/(样本总数)）。
    但是类条件概率的估计往往非常困难，因为类条件概率的密度函数包含了一个随机变量的全部信息，而且概率密度函数的估计还受到样本数据数量的限制以及特征向量维度的限制。因此我们需要将概率密度的估计问题转化为参数的估计问题。选取合适的概率密度函数模型，在样本区域足够大时，就可以得到较为准确的参数估计值。

样本修正
    如果一个给定的类和特征值在训练集中没有一起出现过，那么基于频率的估计下该概率将为零，与其他概率相乘时将会把其他概率的信息统统去除。为了避免这种情况的产生，所以常常要求要对每个小类样本的概率估计进行修正，以保证不会出现有为零的概率出现。

争论
    贝叶斯学派提出了贝叶斯公式与主观概率，他们认为参数可以是一个分布，并且可以由我们的经验赋予先验，也就是说，贝叶斯概率模型可以通过将参数赋予一个方差很大的先验分布，使估计的参数能够让后验概率的方差更小。而概率学派主张大数定律，认为参数应该是一个确定的值而不应该具有随机性。
    而在贝叶斯概率模型简化为朴素贝叶斯的过程中，这种不准确性得到了进一步的放大，为了使算法的复杂度得到简化，假定样本的每个特征都与其他特征不相关，这意味着每个特征的分布都可以独立地被当做一维分布来估计，减轻了由于维数灾带来的阻碍,当样本的特征个数增加时就不需要使样本规模呈指数增长。
但是这样假设带来的结果是不同特征间的相关性被抹除，在这个基础上，解决问题的方案为引入正则化，从贝叶斯学派的角度来看，这相当于在权重上引入先验，但是在概率学派看来，这是错误的做法。   
    尽管实际上独立假设常常是不准确的，但朴素贝叶斯分类器的若干特性让其在实践中能够获取令人惊奇的效果。例如，朴素贝叶斯分类器中，依据最大后验概率决策规则只要正确类的后验概率比其他类要高就可以得到正确的分类。所以不管概率估计轻度的甚至是严重的不精确都不影响正确的分类结果。在这种方式下，分类器可以有足够的鲁棒性去忽略朴素贝叶斯概率模型上存在的缺陷。
    
实例
    在日常生活中，我们常常利用朴素贝叶斯分类来做文本分类。其中一个基于内容的文本分类问题，是判断邮件是否为垃圾邮件。假设现在只有两个相互独立的类别，分别为垃圾邮件和非垃圾邮件，邮件又有不同的特征，例如是否具备垃圾邮件中频繁出现的单词。由此基于朴素贝叶斯概率来对邮件进行分类，就能够取得很好的效果。
    此外，可以举一个简单的例子来说明
    症状      职业       疾病
    
    打喷嚏    护士       感冒
    打喷嚏    农夫       过敏
    头痛      建筑工人   脑震荡
    头痛      建筑工人   感冒
    打喷嚏    教师       感冒
    头痛      教师       脑震荡
    在这个例子中给出的为训练数据集，类别集合C={感冒，过敏，脑震荡}，项集合I中有六个元素，每个元素含有两个特征属性，分别为职业和症状。
    假设来了一个打喷嚏的建筑工人，他可能是患了什么病？
    令y1为感冒，y2为过敏，y3为脑震荡；a1为症状，a1={打喷嚏，头痛}；a2为职业，a2={护士，农夫，建筑工人，教师}。
    计算公式为p(yi|x)=p(x|yi)p(yi)/p(x)，其中p(x|yi)=p(a1|yi)p(a2|yi)，在本例中，a1为打喷嚏，a2为建筑工人：

    P(感冒|打喷嚏x建筑工人)
= P(打喷嚏x建筑工人|感冒) x P(感冒) / P(打喷嚏x建筑工人)
=P(打喷嚏|感冒) x P(建筑工人|感冒) x P(感冒) / P(打喷嚏) x P(建筑工人)
=2/3 X 1/3 X 1/2 / 1/2 x 1/3=2/3
    同理可以算出P(过敏|打喷嚏x建筑工人)和P(脑震荡|打喷嚏x建筑工人)
